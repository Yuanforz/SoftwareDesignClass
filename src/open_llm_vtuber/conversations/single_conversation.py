from typing import Union, List, Dict, Any, Optional
import asyncio
import json
from loguru import logger
import numpy as np

from .conversation_utils import (
    create_batch_input,
    process_agent_output,
    send_conversation_start_signals,
    process_user_input,
    finalize_conversation_turn,
    cleanup_conversation,
    EMOJI_LIST,
)
from .types import WebSocketSend
from .tts_manager import TTSTaskManager
from ..chat_history_manager import store_message
from ..service_context import ServiceContext

# Import necessary types from agent outputs
from ..agent.output_types import SentenceOutput, AudioOutput


async def process_single_conversation(
    context: ServiceContext,
    websocket_send: WebSocketSend,
    client_uid: str,
    user_input: Union[str, np.ndarray],
    images: Optional[List[Dict[str, Any]]] = None,
    session_emoji: str = np.random.choice(EMOJI_LIST),
    metadata: Optional[Dict[str, Any]] = None,
    wake_word_config: Optional[Dict[str, Any]] = None,
) -> str:
    """Process a single-user conversation turn

    Args:
        context: Service context containing all configurations and engines
        websocket_send: WebSocket send function
        client_uid: Client unique identifier
        user_input: Text or audio input from user
        images: Optional list of image data
        session_emoji: Emoji identifier for the conversation
        metadata: Optional metadata for special processing flags
        wake_word_config: Optional wake word configuration for voice input

    Returns:
        str: Complete response text
    """
    # Create TTSTaskManager for this conversation
    tts_manager = TTSTaskManager()
    full_response = ""  # Initialize full_response here

    try:
        # Process user input first (before sending "Thinking..." to check wake word)
        input_text = await process_user_input(
            user_input, context.asr_engine, websocket_send, wake_word_config
        )

        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆï¼ˆè¯­éŸ³è¯†åˆ«å¯èƒ½è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œæˆ–å”¤é†’è¯æ£€æµ‹æœªé€šè¿‡ï¼‰
        if not input_text:
            logger.info("Empty input text, ending conversation chain early")
            # é™é»˜ç»“æŸï¼Œä¸å‘é€ä»»ä½•æ¶ˆæ¯
            return ""
        
        # è¯­éŸ³æç¤ºè¯æ³¨å…¥ï¼šå¦‚æœæ˜¯è¯­éŸ³è¾“å…¥ä¸”å¯ç”¨äº†æ³¨å…¥åŠŸèƒ½
        is_voice_input = isinstance(user_input, np.ndarray)
        voice_prompt_injection = wake_word_config and wake_word_config.get("voice_prompt_injection", False)
        
        # ç”¨äºå‘é€ç»™æ¨¡å‹çš„æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«æ³¨å…¥æç¤ºï¼‰
        model_input_text = input_text
        if is_voice_input and voice_prompt_injection:
            voice_context_prompt = "ã€ä»¥ä¸‹æ˜¯è¯­éŸ³è¾“å…¥è½¬è¯‘ï¼Œå¯èƒ½å­˜åœ¨è°éŸ³å­—æˆ–è¯†åˆ«è¯¯å·®ï¼Œè¯·ç†è§£åŸæ„ã€‘"
            model_input_text = f"{voice_context_prompt}\n{input_text}"
            logger.info(f"Voice prompt injected for model input")

        # Now send initial signals (only after confirming valid input)
        await send_conversation_start_signals(websocket_send)
        logger.info(f"New Conversation Chain {session_emoji} started!")

        # Create batch input (ä½¿ç”¨å¯èƒ½æ³¨å…¥äº†è¯­éŸ³æç¤ºçš„æ–‡æœ¬)
        batch_input = create_batch_input(
            input_text=model_input_text,
            images=images,
            from_name=context.character_config.human_name,
            metadata=metadata,
        )

        # Store user message (ä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼Œä¸åŒ…å«æ³¨å…¥æç¤º)
        skip_history = metadata and metadata.get("skip_history", False)
        if context.history_uid and not skip_history:
            store_message(
                conf_uid=context.character_config.conf_uid,
                history_uid=context.history_uid,
                role="human",
                content=input_text,  # å­˜å‚¨åŸå§‹æ–‡æœ¬
                name=context.character_config.human_name,
            )

        if skip_history:
            logger.debug("Skipping storing user input to history (proactive speak)")

        logger.info(f"User input: {input_text}")
        if images:
            logger.info(f"With {len(images)} images")

        try:
            # agent.chat yields Union[SentenceOutput, Dict[str, Any]]
            agent_output_stream = context.agent_engine.chat(batch_input)

            async for output_item in agent_output_stream:
                if (
                    isinstance(output_item, dict)
                    and output_item.get("type") == "tool_call_status"
                ):
                    # Handle tool status event: send WebSocket message
                    output_item["name"] = context.character_config.character_name
                    logger.debug(f"Sending tool status update: {output_item}")

                    await websocket_send(json.dumps(output_item))

                elif isinstance(output_item, (SentenceOutput, AudioOutput)):
                    # Handle SentenceOutput or AudioOutput
                    response_part = await process_agent_output(
                        output=output_item,
                        character_config=context.character_config,
                        live2d_model=context.live2d_model,
                        tts_engine=context.tts_engine,
                        websocket_send=websocket_send,  # Pass websocket_send for audio/tts messages
                        tts_manager=tts_manager
                    )
                    # Ensure response_part is treated as a string before concatenation
                    response_part_str = (
                        str(response_part) if response_part is not None else ""
                    )
                    full_response += response_part_str  # Accumulate text response
                else:
                    logger.warning(
                        f"Received unexpected item type from agent chat stream: {type(output_item)}"
                    )
                    logger.debug(f"Unexpected item content: {output_item}")

        except Exception as e:
            logger.exception(
                f"Error processing agent response stream: {e}"
            )  # Log with stack trace
            await websocket_send(
                json.dumps(
                    {
                        "type": "error",
                        "message": f"Error processing agent response: {str(e)}",
                    }
                )
            )
            # full_response will contain partial response before error
        # --- End processing agent response ---

        # å¤„ç†å‰©ä½™çš„éŸ³é¢‘åˆå¹¶ç¼“å†²å†…å®¹ï¼ˆä»…åœ¨ Step TTS åˆå¹¶æ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰
        await tts_manager.flush_remaining(
            live2d_model=context.live2d_model,
            tts_engine=context.tts_engine,
            websocket_send=websocket_send,
        )

        # Wait for any pending TTS tasks
        if tts_manager.task_list:
            await asyncio.gather(*tts_manager.task_list)
            await websocket_send(json.dumps({"type": "backend-synth-complete"}))

        await finalize_conversation_turn(
            tts_manager=tts_manager,
            websocket_send=websocket_send,
            client_uid=client_uid,
        )

        if context.history_uid and full_response:  # Check full_response before storing
            store_message(
                conf_uid=context.character_config.conf_uid,
                history_uid=context.history_uid,
                role="ai",
                content=full_response,
                name=context.character_config.character_name,
                avatar=context.character_config.avatar,
            )
            logger.info(f"AI response: {full_response}")

        return full_response  # Return accumulated full_response

    except asyncio.CancelledError:
        logger.info(f"ğŸ¤¡ğŸ‘ Conversation {session_emoji} cancelled because interrupted.")
        raise
    except Exception as e:
        logger.error(f"Error in conversation chain: {e}")
        await websocket_send(
            json.dumps({"type": "error", "message": f"Conversation error: {str(e)}"})
        )
        raise
    finally:
        cleanup_conversation(tts_manager, session_emoji)
